---
title: "sensitivity_LHS"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(sensitivity)
library(tidyverse)
library(lhs)
library(purrr)
library(here)
```

# Example of using Latin Hypercube sampling for sensitivity analysis

Lets look at our almond yield example

```{r LHS}
# for formal sensitivity analysis it is useful to describe output in
# several summary statistics - how about mean, max and min yield
source(here("R/compute_almond_yield.R"))


# set a random seed to make things 'random'
set.seed(1)

# which parameters
pnames <- c("Tmincoeff1", "Tmincoeff2", "Pcoeff1", "Pcoeff2", "intercep")

# how many parameters
npar <- length(pnames)
# how many samples
nsample <- 100

# create our latin hyper cube of quantiles for each parameter
parm_quant <- randomLHS(nsample, npar)
colnames(parm_quant) <- pnames



# choose distributions for parameters - this would come from
# what you know about the likely range of variation
# then use our random samples to pick the quantiles

# first set up a data frame to hold the samples for each parameter
parm <- as.data.frame(matrix(nrow = nrow(parm_quant), ncol = ncol(parm_quant)))
colnames(parm) <- pnames
# for each parameter pick samples based on lhs quantiles
# I'm using several examples normal distribution (with 10% standard deviation) and uniform with +- 10%
# in reality I should pick distribution from knowledge about uncertainty in parameters

# to make it easy to change i'm setting standard deviation / range variation to a variable
pvar <- 10

parm[, "Tmincoeff1"] <- qnorm(parm_quant[, "Tmincoeff1"], mean = -0.015, sd = 0.015 / pvar)
parm[, "Tmincoeff2"] <- qnorm(parm_quant[, "Tmincoeff2"], mean = -0.0046, sd = 0.0046 / pvar)

# for uniform I'm using +- 10%
parm[, "Pcoeff1"] <- qunif(parm_quant[, "Pcoeff1"], min = -0.07 - 0.07 / pvar, max = -0.07 + 0.07 / pvar)
parm[, "Pcoeff2"] <- qunif(parm_quant[, "Pcoeff2"], min = -0.0043 - 0.0043 / pvar, max = -0.0043 + 0.0043 / pvar)

parm[, "intercep"] <- qnorm(parm_quant[, "intercep"], mean = 0.28, sd = 0.28 / pvar)
# note I could also index by column number by names keep things more clear, fewer mistakes
# parm[,5] = qnorm(parm_quant[,5], mean=0.28, sd=0.28/pvar)

head(parm)
```


# Run model for parameter sets

* We will do this in R

* We can use **pmap** to efficiently run our model for all of our parameter sets

# Examining Output

In this case we have yields over time and summaries that include

* mean
* max
* min 

Lets generate these values for each parameter set
```{r almondsens2}
# read in the input data
clim <- read.table(here("data/clim.txt"), header = T)

# lets now run our model for all of the parameters generated by LHS
# pmap is useful here - it is a map function that uses the actual names of input parameters

yields <- parm %>% pmap(compute_almond_yield, clim = clim) # Want to vary the inputs for the ### parm ### 

# notice that what pmap returns is a list
head(yields)

# turn results in to a dataframe for easy display/analysis
yieldsd <- yields %>% map_dfr(`[`, c("maxyield", "minyield", "meanyield"))
```
#  Plotting 

Plot relationship between parameter and output
to understand how uncertainty in parameter impacts the output to determine over what ranges of the parameter uncertainty is most important (biggest effect)


* Use a box plot (of output)
to graphically show the impact of uncertainty on output of interest

* To see more of the distribution - graph the cumulative distribution
  * high slope, many values in that range
  * low slope, few values in that range
  * constant slope, even distribution
  
* Scatterplots against parameter values


```{r senplot}
# add uncertainty bounds on our estimates
tmp <- yieldsd %>% gather(value = "value", key = "yield")
ggplot(tmp, aes(yield, value, col = yield)) +
  geom_boxplot() +
  labs(y = "Yield (as anomoly)")

# note that you don't see the ranges because of the scale (min yield anomoly much smaller than max) - here's a more informative way to graph
ggplot(tmp, aes(yield, value, col = yield)) +
  geom_boxplot() +
  labs(y = "Yield (as anomoly)") +
  facet_wrap(~yield, scales = "free")


# cumulative distribution
ggplot(yieldsd, aes(maxyield)) +
  stat_ecdf()


# plot parameter sensitivity
# a bit tricky but nice way to make it easy to plot all parameters against all values
tmp <- cbind.data.frame(yieldsd, parm)
tmp2 <- tmp %>% gather(maxyield, minyield, meanyield, value = "yvalue", key = "yieldtype")
tmp3 <- tmp2 %>% gather(-yvalue, -yieldtype, key = "parm", value = "parmvalue")
ggplot(tmp3, aes(parmvalue, yvalue, col = yieldtype)) +
  geom_point() +
  facet_wrap(~ yieldtype * parm, scales = "free", ncol = 5)
```

Quantifying

# Correlation and Pcc for Maximum Yield
```{r quantifying}


# simple correlation coefficients
result <- map(parm, cor.test, y = yieldsd$maxyield)
result$Tmincoeff1
result$Tmincoeff2
result$Pcoeff10
result$Pcoeff2
result$intercep

# extract just the correlation from results list
justR2 <- result %>% map_df("estimate")
justR2$pname = names(parm)
justR2

# extract just the confidence interval from results list
justconf <- result %>% map_df("conf.int")
justconf
```

## Parital Rank Correlation Coefficients for Maximum Yield

we can use *pcc* to compute these for us
```{r quantifying2}
# partial regression rank coefficients
senresult_rank <- pcc(parm, yieldsd$maxyield, rank = TRUE)
senresult_rank

```
## plot results
```{r quantifyingp2}
# plot results
plot(senresult_rank)
```

## try computing for min yield, and mean yields

think about why PRCC are different for different output

### Correlation and pcc for min yield
```{r}


# simple correlation coefficients
result_min <- map(parm, cor.test, y = yieldsd$minyield)
result_min$Tmincoeff1
result_min$Tmincoeff2
result_min$Pcoeff10
result_min$Pcoeff2
result_min$intercep

# extract just the correlation from results list
justR2_min <- result_min %>% map_df("estimate")
justR2_min$pname = names(parm)
justR2_min

# extract just the confidence interval from results list
justconf_min <- result_min %>% map_df("conf.int")
justconf_min
```

